{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Tensorflow Neural Network\n",
    "- Simple implementation of neural network on tensorflow using mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist dataset\n",
    "train_data = pd.read_csv('data/mnist_train.csv')\n",
    "test_data = pd.read_csv('data/mnist_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>1x10</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  1x10  ...  28x19  28x20  \\\n",
       "0    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "1    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "2    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "3    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "4    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_data.drop(columns=['label'])\n",
    "Y = train_data['label']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test data\n",
    "scaler = MinMaxScaler()\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "#scale data to 0-1 range of values\n",
    "x_train_scaled = scaler.fit_transform(x_train.astype(np.float64))\n",
    "x_test_scaled = scaler.fit_transform(x_test.astype(np.float64))\n",
    "#convert labels as int\n",
    "y_train  = y_train.astype(int)\n",
    "y_test  = y_test.astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "    \n",
    "# one hot encode y values\n",
    "y_train = np.array(y_train)\n",
    "y_train = dense_to_one_hot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jonvero/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-8-09b42f61133c>:48: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 10 / Loss: 2.003438232330289 / Accuracy: 0.3120950838414636\n",
      "Epoch: 20 / Loss: 1.8336946956995053 / Accuracy: 0.5286061356707311\n",
      "Epoch: 30 / Loss: 1.7285399384251448 / Accuracy: 0.6286918826219509\n",
      "Epoch: 40 / Loss: 1.696612792952759 / Accuracy: 0.6613471798780483\n",
      "Epoch: 50 / Loss: 1.679138533225874 / Accuracy: 0.7014338795731713\n",
      "Epoch: 60 / Loss: 1.6650271101332297 / Accuracy: 0.7100562118902446\n",
      "Epoch: 70 / Loss: 1.6551846278513336 / Accuracy: 0.7327791539634161\n",
      "Epoch: 80 / Loss: 1.6461573312195334 / Accuracy: 0.7540491615853672\n",
      "Epoch: 90 / Loss: 1.640423050559148 / Accuracy: 0.7645769817073192\n",
      "Epoch: 100 / Loss: 1.6342068225508806 / Accuracy: 0.7774390243902453\n",
      "Epoch: 110 / Loss: 1.629229284822941 / Accuracy: 0.7835604039634164\n",
      "Epoch: 120 / Loss: 1.6255905377428703 / Accuracy: 0.7857278963414647\n",
      "Epoch: 130 / Loss: 1.6224110807950924 / Accuracy: 0.7881335746951242\n",
      "Epoch: 140 / Loss: 1.6198291762209522 / Accuracy: 0.7973275533536596\n",
      "Epoch: 150 / Loss: 1.6176596694603216 / Accuracy: 0.7993044969512213\n",
      "Epoch: 160 / Loss: 1.6134986394062283 / Accuracy: 0.8075219131097583\n",
      "Epoch: 170 / Loss: 1.6124469579719924 / Accuracy: 0.8095226753048796\n",
      "Epoch: 180 / Loss: 1.6105945624592823 / Accuracy: 0.8134765625000018\n",
      "Epoch: 190 / Loss: 1.6089967541941776 / Accuracy: 0.8123809070121971\n",
      "Epoch: 200 / Loss: 1.6068092889902068 / Accuracy: 0.8186928353658552\n",
      "Epoch: 210 / Loss: 1.6056057887106407 / Accuracy: 0.816882621951221\n",
      "Epoch: 220 / Loss: 1.6045656682151115 / Accuracy: 0.820336318597563\n",
      "Epoch: 230 / Loss: 1.6030724619219923 / Accuracy: 0.8215034298780506\n",
      "Epoch: 240 / Loss: 1.602977311465799 / Accuracy: 0.8249571265243916\n",
      "Epoch: 250 / Loss: 1.6007034845831905 / Accuracy: 0.8274342606707329\n",
      "Epoch: 260 / Loss: 1.599746061716137 / Accuracy: 0.8284584603658555\n",
      "Epoch: 270 / Loss: 1.5980629017803722 / Accuracy: 0.8289110137195129\n",
      "Epoch: 280 / Loss: 1.5979227311000601 / Accuracy: 0.8302210365853671\n",
      "Epoch: 290 / Loss: 1.5961130110592379 / Accuracy: 0.8344845655487815\n",
      "Epoch: 300 / Loss: 1.5961522098358063 / Accuracy: 0.8320788871951227\n",
      "Epoch: 310 / Loss: 1.5952018077780568 / Accuracy: 0.8341034679878063\n",
      "Epoch: 320 / Loss: 1.5938785159733233 / Accuracy: 0.8356993140243917\n",
      "Epoch: 330 / Loss: 1.5936204271345595 / Accuracy: 0.8351514862804885\n",
      "Epoch: 340 / Loss: 1.5927775078793855 / Accuracy: 0.8371522484756114\n",
      "Epoch: 350 / Loss: 1.5921182656070072 / Accuracy: 0.837962080792684\n",
      "Epoch: 360 / Loss: 1.5913079253784055 / Accuracy: 0.839534108231708\n",
      "Epoch: 370 / Loss: 1.5900549321639827 / Accuracy: 0.841487233231708\n",
      "Epoch: 380 / Loss: 1.5891869068145752 / Accuracy: 0.8433450838414648\n",
      "Epoch: 390 / Loss: 1.5890826532026618 / Accuracy: 0.8431068978658544\n",
      "Epoch: 400 / Loss: 1.5898368925583064 / Accuracy: 0.8414395960365867\n",
      "Epoch: 410 / Loss: 1.5881148878394116 / Accuracy: 0.8452029344512212\n",
      "Epoch: 420 / Loss: 1.5871904879444974 / Accuracy: 0.8440120045731715\n",
      "Epoch: 430 / Loss: 1.5861958298014438 / Accuracy: 0.8473466082317086\n",
      "Epoch: 440 / Loss: 1.5871846750378593 / Accuracy: 0.8456793064024405\n",
      "Epoch: 450 / Loss: 1.5873037602116413 / Accuracy: 0.8462747713414651\n",
      "Epoch: 460 / Loss: 1.58540677660849 / Accuracy: 0.8483469893292698\n",
      "Epoch: 470 / Loss: 1.5853113990004448 / Accuracy: 0.8463700457317089\n",
      "Epoch: 480 / Loss: 1.5852934183870862 / Accuracy: 0.8497284679878064\n",
      "Epoch: 490 / Loss: 1.5845312763278077 / Accuracy: 0.8487519054878062\n",
      "Epoch: 500 / Loss: 1.5843716714440317 / Accuracy: 0.8508479420731723\n"
     ]
    }
   ],
   "source": [
    "# create raw neural network\n",
    "\n",
    "#define hyper-parameters\n",
    "epochs = 500\n",
    "learning_rate = 0.0001\n",
    "\n",
    "#define network architecture\n",
    "input_size = x_train.shape[1]\n",
    "h1_size = 256\n",
    "h2_size = 256\n",
    "batch_size = 64\n",
    "output_size = len(np.unique(np.array(Y)))\n",
    "total_batch = int(len(x_train_scaled) / batch_size)\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, input_size])\n",
    "y = tf.placeholder(\"float\", [None, output_size])\n",
    "keep_prob = tf.placeholder(\"float\", None) # dropout\n",
    "#create weights and biases\n",
    "\n",
    "#weights of hidden layers\n",
    "w_h1= tf.Variable(tf.random.normal([input_size,h1_size]))\n",
    "w_h2 = tf.Variable(tf.random.normal([h1_size,h2_size ]))\n",
    "w_output = tf.Variable(tf.random.normal([h2_size,output_size]))\n",
    "#biases of hidden layers\n",
    "b_h1 = tf.Variable(tf.random.normal([h1_size]))\n",
    "b_h2 = tf.Variable(tf.random.normal([h2_size]))\n",
    "b_output = tf.Variable(tf.random.normal([output_size]))\n",
    "\n",
    "#create layer forward function\n",
    "def nn_layer(x_input,w,b,acti,drop=None):\n",
    "    z = tf.add(tf.matmul(x_input,w),b) # linear function\n",
    "    if acti == 'relu':\n",
    "        a = tf.nn.relu(z)\n",
    "    elif acti == 'sigmoid':\n",
    "        a = tf.nn.sigmoid(z)\n",
    "    elif acti == 'tanh':\n",
    "        a = tf.nn.tanh(z)\n",
    "    #perform dropout\n",
    "    #a = tf.nn.dropout(a,drop)\n",
    "    return a\n",
    "\n",
    "# build neural network\n",
    "\n",
    "a_1 = nn_layer(x,w_h1,b_h1,'relu') #layer activation 1\n",
    "a_2 = nn_layer(a_1,w_h2,b_h2,'relu') #layer activation 2\n",
    "output = nn_layer(a_2,w_output,b_output,'sigmoid') #layer activation output\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output,labels=y)) #compute loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) #create optimizer\n",
    "train_opti = optimizer.minimize(loss)\n",
    "#compute accuracy\n",
    "correct_pred = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# train neural network\n",
    "init = tf.global_variables_initializer() #initialize all tf variables \n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for e in range(epochs):\n",
    "    avg_cost = 0\n",
    "    avg_acc = 0\n",
    "    #train per batch data\n",
    "    for i in range(1,total_batch+1):\n",
    "        #create training data batches\n",
    "        batch_start = i *batch_size\n",
    "        batch_end = batch_start + batch_size\n",
    "        if batch_end > x_train_scaled.shape[0]:\n",
    "            batch_end = x_train_scaled.shape[0]\n",
    "        batch_x,batch_y = x_train_scaled[batch_start:batch_end,:],y_train[batch_start:batch_end,:]\n",
    "        \n",
    "        #train neural net\n",
    "        _,loss_val,acc = sess.run([train_opti,loss,accuracy], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        avg_cost += loss_val /total_batch\n",
    "        avg_acc += acc /total_batch\n",
    "    if (e+1) % 10 == 0:\n",
    "        print(f\"Epoch: {e+1} / Loss: {avg_cost} / Accuracy: {avg_acc}\") \n",
    "    \n",
    "sess.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
